\documentclass[12pt, titlepage]{article}
\usepackage{amsmath, mathtools}
\usepackage{siunitx}
\usepackage{colortbl}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\newcounter{refnum} %Reference Number
\newcommand{\retherefnum}{REF\therefnum}
\newcommand{\reref}[1]{\ref{#1}}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for Bridge Corrosion: A Chloride Exposure Prediction Model} 
\author{Cynthia Liu}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{2.5cm}p{1.5cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb 9, 2024 & 1.0 & Initial release\\
Mar 7, 2024 & 2.0 & Modify according to feedback from peer review\\
\bottomrule
\end{tabularx}

% ~\\
% \wss{The intention of the VnV plan is to increase confidence in the software.
% However, this does not mean listing every verification and validation technique
% that has ever been devised.  The VnV plan should also be a \textbf{feasible}
% plan. Execution of the plan should be possible with the time and team available.
% If the full plan cannot be completed during the time available, it can either be
% modified to ``fake it'', or a better solution is to add a section describing
% what work has been completed and what work is still planned for the future.}

% \wss{The VnV plan is typically started after the requirements stage, but before
% the design stage.  This means that the sections related to unit testing cannot
% initially be completed.  The sections will be filled in after the design stage
% is complete.  the final version of the VnV plan should have all sections filled
% in.}

\newpage

\tableofcontents

\listoftables
% \wss{Remove this section if it isn't needed}

% \listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}


\subsection{Table of Symbols}
\noindent \begin{tabular}{l l p{12cm}} \toprule
\textbf{symbol} & \textbf{unit} & \textbf{description}\\
\midrule 
$C_s$ & $kg/m^3/vehicle$ & chloride on the bridge substructure\\
$C_{s_{air}}$ & $kg/m^3/vehicle$ & chloride sprayed and splashed per unit air volume per vehicle\\
$h_{app}$ & $m$ & daily water film thickness on the road\\
$h_{total}$ & $m$ & the total snowfall during a winter season\\
$M_{app}$ & $kg/m^2$ & deicing salts quantity applied per day\\
$M_{total}$ & $kg/m^2$ & total amount of deicing salts quantity over winter\\
$SD_{total}$ & $kg/m^{3}/vehicle$ & spray density kicked up by each passing truck\\
$SD_{total~cl}$ & $kg/m^3/vehicle$ & mass of chloride ions per unit air volume\\
$t_1$ & days & number of days with snowfall\\
$t_2$ & days & number of days with snow melting\\
\bottomrule
\end{tabular}



\subsection{Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 

  AADT & Annual Average Daily Traffic\\
  AADTT & Annual Average Daily Truck Traffic \\
  BC & Bridge Corrosion\\
  CI & Continuous Integration\\
  FR & Functional Requirement\\
  MG & Module Guide\\
  MIS & Module Interface Specification\\
  NFR & Non-functional Requirement\\
  R & Requirement\\
  SRS & Software Requirements Specification\\
  T & Test\\
  VnV & Verification and Validation\\
  
  \bottomrule
\end{tabular}\\

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%   \citep{SRS} tables, if appropriate}

% \wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

\section{General Information}

This document provides the road-map of the verification and validation plan for Bridge Corrosion (BC), and ensure the requirements and goals of the program mentioned in SRS. The organization of this document starts with the general information and verification plans, followed by the system test description for functional and non-functional requirements. Then, it includes the unit test which would not be filled in until after the MIS.

\subsection{Summary}

% \wss{Say what software is being tested.  Give its name and a brief overview of
%   its general functions.}
The software BC, provides predictive trends for chloride exposure based on user-input coordinates within the province of Ontario. The software utilizes climate and traffic models to build a chloride exposure database, and search for the trend when user input location data.

\subsection{Objectives}
The objective of this document is to build confidence in the software's correctness and increase its realiability. All functional requirements and non-functional requirements mentioned in SRS are tested. We will try to cover a usability test if we have time and resource. We also assume the climate and traffic models we generate online have been verified by their implementation team so their accuracy is ensured.


% \wss{State what is intended to be accomplished.  The objective will be around
%   the qualities that are most important for your project.  You might have
%   something like: ``build confidence in the software correctness,''
%   ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
%   just those that are most important.}

% \wss{You should also list the objectives that are out of scope.  You don't have 
% the resources to do everything, so what will you be leaving out.  For instance, 
% if you are not going to verify the quality of usability, state this.  It is also 
% worthwhile to justify why the objectives are left out.}

% \wss{The objectives are important because they highlight that you are aware of 
% limitations in your resources for verification and validation.  You can't do everything, 
% so what are you going to prioritize?  As an example, if your system depends on an 
% external library, you can explicitly state that you will assume that external library 
% has already been verified by its implementation team.}

\subsection{Relevant Documentation}
The relevant documentation includes \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/ProblemStatementAndGoals/ProblemStatement.pdf}{Problem Statement} which is the proposed idea, \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/SRS/SRS.pdf}{Software Requirements Specifications} which outlines the requirement of the software. It would also be related to \href{}{VnV Report} which is a conclusion for validation and verification after the software is developed.

 
% \wss{Reference relevant documentation.  This will definitely include your SRS
%   and your other project documents (design documents, like MG, MIS, etc).  You
%   can include these even before they are written, since by the time the project
%   is done, they will be written.}


% \wss{Don't just list the other documents.  You should explain why they are relevant and 
% how they relate to your VnV efforts.}

\section{Plan}
The VnV plan begins by introducing the verification and validation team (section \ref{VnVT}), followed by the verification plans for the SRS (section \ref{SRSVP}) and design (section \ref{DVP}). Subsequently, the verification plans for the VnV Plan (section \ref{VnVP}) and implementation (section \ref{IVP}) are presented. In the end, there are sections on automated testing and verification tools (Section \ref{ATVT}) and the software validation plan (section \ref{SVP}).

% \wss{Introduce this section.   You can provide a roadmap of the sections to
%   come.}

\subsection{Verification and Validation Team}\label{VnVT}

% \wss{Your teammates.  Maybe your supervisor.
%   You should do more than list names.  You should say what each person's role is
%   for the project's verification.   A table is a good way to summarize this information.}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{ %
    \begin{tabular}{ |l|l|p{2cm}|p{5cm}| } 
\hline        

	 Name & Document & Role & Description \\
\hline
      Dr. Spencer Smith & All & Instructor/ Reviewer & Review all the documents.  \\ \hline
      Mingsai Xu & - & Domain Expert & Provide theory support for the software.   \\ \hline     	  
	 Cynthia Liu & All & Author & Create all the documents, implement the software, and verify the implementation according to the VnV plan. \\ \hline
      Phil Du & All & Domain Expert Reviewer & Review all the documents. \\ \hline
      Hunter Ceranic & SRS & Secondary Reviewer & Review the SRS document. \\ \hline
      Fasil Cheema & VnV plan & Secondary Reviewer & Review the VnV plan.\\ \hline
      Fatemeh Norouziani & MG + MIS & Secondary Reviewer & Review the MG and MIS document. \\ \hline
\hline
\end{tabular} %
}
\caption{Verification and Validation Team}
\label{Table:VnVT}
\end{table}


\subsection{SRS Verification Plan}\label{SRSVP}
The SRS verification is done by peer review with classmates (Phil Du and Hunter Ceranic), giving suggestions by creating issues in Github. The author (Cynthia Liu) is responsible for reviewing and addressing the issues.\\
The SRS verification is also reviewed by Dr. Spencer Smith to offer any suggestions and feedbacks.\\
There is a \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS checklist} designed by Dr. Spencer Smith available to use if desired.
% \wss{List any approaches you intend to use for SRS verification.  This may include
%   ad hoc feedback from reviewers, like your classmates, or you may plan for 
%   something more rigorous/systematic.}

% \wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}\label{DVP}
The design verification, including the module guide (MG) and module interface specification (MIS), will be verfied by Phil Du and Fatemeh Norouziani. Dr. Spencer Smith will also review both documents. Reviewers will give feedbacks through Github issues and the author need to address them. The \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/Checklists/MG-Checklist.pdf}{MG checklist} and \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/Checklists/MIS-Checklist.pdf}{MIS checklist} designed by Dr. Spencer Smith could act as a help for reviewers.
% \wss{Plans for design verification}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}\label{VnVP}
The VnV plan is first created and verfied by the author, then turns to the team members to give any suggests by Github issues. The reviewers will access the \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/Checklists/VnV-Checklist.pdf}{VnV checklist} designed by Dr. Spencer Smith for reference.
% \wss{The verification and validation plan is an artifact that should also be
% verified.  Techniques for this include review and mutation testing.}

% \wss{The review will include reviews by your classmates}

% \wss{Create a checklists?}

\subsection{Implementation Verification Plan}\label{IVP}
The implementation would be verified by testing the functional requirements and non-functional requirements in section \ref{STD}. The unit tests will also be conducted as detailed in section \ref{UTD}. If possible, a code walkthrough could happen.
% \wss{You should at least point to the tests listed in this document and the unit
%   testing plan.}

% \wss{In this section you would also give any details of any plans for static
%   verification of the implementation.  Potential techniques include code
%   walkthroughs, code inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}\label{ATVT}
The first part of the software, generating the database through climate and traffic model, will be done in MATLAB. The second part of finding the data for the input location, will be done in R. The automated testing will be done in MATLAB and R correspondingly. During the coding, \href{https://www.mathworks.com/help/matlab/ref/mlint.html}{mlint} and \href{https://lintr.r-lib.org/}{lintr} will be used as a static code analysis. The Github Actions is used for continuous integration, that the CI work flow will run when new code is pushed to the repository. It will also run the CI tests and provide the results of each test in the pull request, so we can see whether the change in the branch introduces an error.
% \wss{What tools are you using for automated testing.  Likely a unit testing
%   framework and maybe a profiling tool, like ValGrind.  Other possible tools
%   include a static analyzer, make, continuous integration tools, test coverage
%   tools, etc.  Explain your plans for summarizing code coverage metrics.
%   Linters are another important class of tools.  For the programming language
%   you select, you should look at the available linters.  There may also be tools
%   that verify that coding standards have been respected, like flake9 for
%   Python.}

% \wss{If you have already done this in the development plan, you can point to
% that document.}

% \wss{The details of this section will likely evolve as you get closer to the
%   implementation.}

\subsection{Software Validation Plan}\label{SVP}
Software validation plan is beyond the scope for BC as it may require additional time, expertise, and resources that are not available within the scope of the project. 
% \wss{If there is any external data that can be used for validation, you should
%   point to it here.  If there are no plans for validation, you should state that
%   here.}

% \wss{You might want to use review sessions with the stakeholder to check that
% the requirements document captures the right requirements.  Maybe task based
% inspection?}

% \wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
% be used as an opportunity to validate the requirements.  You should plan on 
% demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
% The feedback from your supervisor will be very useful for improving your project.}

% \wss{For teams without an external supervisor, user testing can serve the same purpose 
% as a Rev 0 demo for the supervisor.}

% \wss{This section might reference back to the SRS verification section.}

\section{System Test Description}\label{STD}
\subsection{Tests for Functional Requirements}
Functional requirements for BC are given in \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/SRS/SRS.pdf}{SRS} section 5.1. There are five functional requirements for BC, R1 is related to the input, R3 is related to the midway calculation, and the other requirements are corresponding to outputs. Section \ref{t_input} describes the input tests related to R1, and section \ref{t_calculation} describes the test related to R3. The output test related to other requirements is discussed in section \ref{t_output}.
\\

% \wss{Subsets of the tests may be in related, so this section is divided into
%   different areas.  If there are no identifiable subsets for the tests, this
%   level of document structure can be removed.}

% \wss{Include a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.}

\subsubsection{Input Test - test\_invalid\_input}\label{t_input}
This section covers R1 of the SRS which includes the input coordinates check for the software. This section also cover the dataset inputs that developers input from models to the software. In most of the cases, the dataset in the models are valid as they have been verified by their developers, but the test cases here still include the scenarios where data may be missing or invalid.

% \wss{It would be nice to have a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.  If a section
%   covers tests for input constraints, you should reference the data constraints
%   table in the SRS.}
		
\paragraph{Functional tests - Input tests - Coordinate}

\begin{center}
\begin{table}[h]
\resizebox{\textwidth}{!}
{ %
    \begin{tabular}{ lccccc }
    \hline
      \multicolumn{2}{c|}{Input (\si[per-mode=symbol] {\degree}) }                            & \multicolumn{2}{c}{Output} \\ 
    
    \hline
        $longitude$   &   $latitude$     &   valid?   &   Error Message \\ \hline
    
       278.9699  & 44.46539 &  Y  & No error message, continue to calcualte output                         \\  \hline
       278.9699  & 41.197 &  N  & Input position not inside Ontario                         \\  \hline
       -84.34  & 44.46539 &  N  & Input position not inside Ontario                         \\  \hline
       -81.0301  & 44.46539 &  Y  & No error message, continue to calcualte output                          \\  \hline
       -74.3  & 45.2 &  Y  &  No error message, continue to calcualte output                       \\   \hline
       278.9699  &  &  N  & Missing latitude                         \\   \hline
      &  44.46539 &  N  & Missing longitude                       \\

    
    \hline
    
    
    \end{tabular} %
}
\caption{Coordinate input tests}
\label{Table:coordinate_test}
\end{table}
\end{center}

\begin{enumerate}

\item{test-input-coordinate\\}
Control: Automatic
					
Initial State: Uninitialized
					
Input: The input column in Table \ref{Table:coordinate_test}.
					
Output: The output column in Table \ref{Table:coordinate_test}. 

Test Case Derivation: The input coordinate need to be a location inside Ontario. The software produces error message for the invalid inputs, for the valid inputs, it will proceed to next step. \wss{Justify the expected value given in the Output field}
					
How test will be performed: The author will create test cases and run automatic test by R.
\end{enumerate}

\paragraph{Functional tests - Input tests - Models}

\begin{center}
\begin{table}[h]
\resizebox{\textwidth}{!}
{ %
    \begin{tabular}{ lccccc }
    \hline
      \multicolumn{3}{c|}{Input (\si[per-mode=symbol] {\degree}) }                            & \multicolumn{2}{c}{Output} \\ \hline
        $h_{total}$   &   $t_1$   & $t_2$  &   valid?   &   Error Message \\ \hline
    
       183.9371  & 109 & 76 &  Y  & No error message, continue to import model data  \\    \hline
       194.6057  & 367 & 255 &  N  &  $t_1$ and $t_2$ must be less than 365  \\    \hline
       119.62  & 53 & -30 &  N  & $t_1$ and $t_2$ need to be positive                 \\    \hline  
         & 109 & 76 &  N  & Data missing for $h_{total}$                \\    \hline  
       183.9371  &  & 76 &   N  & Data missing for  $t_1$                    \\    \hline
       183.9371  & 109 &  &  N  & Data missing for  $t_2$                       \\    \hline
    \end{tabular} %
}
\caption{Climate model input tests}
\label{Table:climate_test}
\end{table}
\end{center}

\begin{enumerate}

\item{test-input-climate\\}

Control: Automatic
					
Initial State: Uninitialized
					
Input: The input column in Table \ref{Table:climate_test}.
					
Output: The output column in Table \ref{Table:climate_test}. 
% \wss{The expected result for the given inputs}

Test Case Derivation: This test case is used when inputing the climate models. There are three parameters that would be used from the model and this test is trying to detect if any of them is missing. If so, it will produce error message. It will also produce error message if there is such data but it is invalid. 
% \wss{Justify the expected value given in the Output field}
					
How test will be performed: The author will pick the sample test case from the model (as in Table \ref{Table:climate_test}) and create automatic test by R.



\begin{center}
\begin{table}[h]
\resizebox{\textwidth}{!}
{ %
    \begin{tabular}{ lccccc }
    \hline
      \multicolumn{2}{c|}{Input (\si[per-mode=symbol] {\degree}) }                            & \multicolumn{2}{c}{Output} \\ 
    
    \hline

        $AADT$   &   $AADTT$   &   valid?   &   Error Message \\ \hline
    
       20176  & 433 &   Y  &  No error message, continue to import model data          \\      \hline
       152  & 8095 &   N  & $AADTT$ should be less than $AADT$                        \\      \hline
         & 152 &   N  & Data missing for $AADT$                        \\      \hline
       8095 & &   N  & Data missing for $AADTT$                        \\      \hline
    
    \end{tabular} %
}
\caption{Traffic model input tests}
\label{Table:traffic_test}
\end{table}
\end{center}

\item{test-input-traffic\\}

Control: Automatic
					
Initial State: Uninitialized

Input: The input column in Table \ref{Table:traffic_test}.
					
Output: The output column in Table \ref{Table:traffic_test}. 
% \wss{The expected result for the given inputs}

Test Case Derivation: This test case is applicable when entering traffic models into the software. There are two parameters that would be used from the model and this test is trying to detect if any of them is missing. If so, it will produce error message. Similarly, if the data exists but is invalid, the software will also generate an error message.
% \wss{Justify the expected value given in the Output field}
					
How test will be performed: The author will pick the sample test case from the model (as in Table \ref{Table:traffic_test}) and create automatic test by R.
\end{enumerate}



\subsubsection{Intermediate Tests - test\_model\_calculation}\label{t_calculation}
\an{This section might be more suitable for the unit test as section 4 is for system level? Will come back to this after MG and MIS.}\\
This section covers R3 of the SRS which includes the check for the middle calculation of the software to generate the database of the software. There are six steps over the calculation process following section 4.4 in \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/SRS/SRS.pdf}{SRS} as the following.
\begin{itemize}
\item Step 1: Calcualte the quantity of deicing salts applied per day with snowfall.
\item Step 2: Calcualte the thickness of melted water per day with snow melting.
\item Step 3: Calculate the water sprayed and splashed by one truck.
\item Step 4: Calculate the chloride ions sprayed and splashed by one truck.
\item Step 5: Calculate the chloride ions sprayed and splashed by all vehicles in one winter season.
\item Step 6: Calculate the deposition of deicing salts on the surface of the bridge substructure.
\end{itemize}

% \wss{It would be nice to have a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.  If a section
%   covers tests for input constraints, you should reference the data constraints
%   table in the SRS.}
		
\paragraph{Functional tests - Intermediate tests - Model Calculations}

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{ %
    \begin{tabular}{|p{2cm}|p{5cm}|p{5cm}|p{3.8cm}|} 
\hline        
       Test case & Input from Model & Input from Previous Steps & Expected Output  \\ \hline
    Step 1& $M_{total}=7.42$, $t_1=106$ & n/a &  $M_{app}=0.07$        \\ \hline
    Step 2 & $h_{total}=206.2145$, $t_1=106$ & n/a &  $h_{app}=1.945$          \\ \hline
    Step 3 & n/a & $h_{app}=1.945$ &  $SD_{total}=97.96$        \\ \hline
    Step 4 & n/a & $SD_{total}=97.96$, $M_{app}=0.07$, $h_{app}=1.945$ & $SD_{total~cl}=0.00216$         \\ \hline
    Step 5 & $AADT=1730$, $AADTT=103$, $t_2=76$ & $SD_{total~cl}=0.00216$ & $C_{s_{air}}=21.7983$         \\ \hline
    Step 6 & n/a & $C_{s_{air}}=21.7983$  & $C_s=18.668$         \\ \hline
\end{tabular} %
}
\caption{Test Case for Model Calculation}
\label{Table:calculation}
\end{table}


\begin{enumerate}

\item{test-calculation\\}
					
Control: Automatic 
					
Initial State: None 
					
Input: The input columns in Table \ref{Table:calculation}. The input are selected from the models or the result value from previous steps. 
					
Output: The output columns in Table \ref{Table:calculation}. The test case is considered to pass if the epsilon between the actual output and the expected output is within a specified threshold. The detail  of threshold will be determined later in the development.

Test Case Derivation: The table shows one sample calculation for the chloride exposure. The expected value in the output column is determined using the equation provided by Mingsai et al ([\reref{ref1}]). At each step, the test case verifies if the calculation for the corresponding step is accurate. If it is, the test proceeds to the next step. 
					
How test will be performed: The automatic test will be performed by R. There will be assert function for each step, and there will be more test cases rather than the one in the table.

\end{enumerate}





\subsubsection{Output Tests - test\_output}\label{t_output}
This section covers R2, R4of the SRS which includes the check for the final outputs.

% \wss{It would be nice to have a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.  If a section
%   covers tests for input constraints, you should reference the data constraints
%   table in the SRS.}
		
\paragraph{Functional tests - Output tests - Chloride exposure}

\begin{enumerate}

\item{test-output-exposure\\}

Control: Automatic
					
Initial State: Uninitialized
					
Input: Coordinate
					
Output: If the input coordinate is within Ontario, it will return a series (length of at lease ten) of two decimal points data showing the chloride exposure at that location.

Test Case Derivation: This test case cover the R2 and R4 of the SRS. The test will be done by checking the length of the output and the precision after decimal points. The expected value should be within the data constraints in the 4.4.7 section of \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/SRS/SRS.pdf}{SRS}.

%  \wss{Justify the expected value given in the Output field}
					
How test will be performed: The automatic test will be performed by R.


\item{test-output-verification\\}

Control: Manual
					
Initial State: Uninitialized
					
Input: Chloride exposure data
					
Output: True or False.

Test Case Derivation: This test case compares the chloride exposure data that the software generates with the real world data for verification. The test result will be true if the derivation is within a threshold. The detail value of the threshold will be determined during development.

%  \wss{Justify the expected value given in the Output field}
					
How test will be performed: Manually compare it with the real world data from previous year. 
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

Non-functional requirements for BC are given in \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/SRS/SRS.pdf}{SRS} section 5.2. There are five non-functional requirements for BC, which are described in the following sections correspondingly.
% \wss{The nonfunctional requirements for accuracy will likely just reference the
%   appropriate functional tests from above.  The test cases should mention
%   reporting the relative error for these tests.  Not all projects will
%   necessarily have nonfunctional requirements related to accuracy}

% \wss{Tests related to usability could include conducting a usability test and
%   survey.  The survey will be in the Appendix.}

% \wss{Static tests, review, inspections, and walkthroughs, will not follow the
% format for the tests given below.}

\subsubsection{NFR: Reliability}\label{t_reliability}
		
\paragraph{Reliability}

The reliability of the software is tested through the test for functional requirements in section 4.1.2 and the unit testing in section \ref{UTD}.					

\subsubsection{NFR: Usability}\label{t_usability}
		
\paragraph{Usability}

\begin{enumerate}

\item{test-usability}

Type: Manual with potential users
					
Initial State: The software is set up and ready for testing.
					
Input/Condition: None
					
Output/Result: Survey result about the user experience with the software.
					
How test will be performed: Observe real users as they perform tasks with the software. Ask them to fill out a survey on their experience, including ease of use, intuitiveness, and any challenges encountered. The survey could be find in \ref{USQ}
\end{enumerate}

		
\subsubsection{NFR: Maintainability}\label{t_maintainability}
		
\paragraph{Maintainability}

\begin{enumerate}

\item{test-maintainability}

Type: Code walkthrough with potential developers
					
Initial State: None
					
Input/Condition: None
					
Output/Result: Feedback from fellow classmates or team members in Table \ref{Table:VnVT}.				
	
How test will be performed: Show the code and the other documents to fellow classmates/potential developers, ask them to explain the code to the author and see how much they could understand. Gather any feedback or questions they have.
\end{enumerate}

		
\subsubsection{NFR: Portability}\label{t_portability}
		
\paragraph{Portability}

\begin{enumerate}

\item{test-portability}

Type: Manual
					
Initial State: None
					
Input/Condition: None
					
Output/Result: Result of compilation.
					
How test will be performed: Firstly try installing R on different operating system, then manually run the software by \textit{Rscript scriptName.R} in terminal or run in IDE to see if it is successful.

\end{enumerate}
		
\subsubsection{NFR: Scalability}\label{t_scalability}
		
\paragraph{Scalability}

\begin{enumerate}

\item{test-scalability}

Type: Automatic
					
Initial State: The software is set up and ready for testing.
					
Input/Condition: Increase the workload by adding more data.
					
Output/Result: Different response time on how the software handles the increased workload.
					
How test will be performed: Record the response time for the software when given increasing amount of dataset, evaluate the different response time.

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
	& R1 & R2 & R3  & R4 & NFR1 & NFR2 & NFR3 & NFR4 & NFR5 \\
\hline
Test \ref{t_input}        & X & & & & & & & & \\ \hline
Test \ref{t_calculation}        & & & X & & & & & & \\ \hline
Test \ref{t_output}        & & X & & X & & & & & \\ \hline
Test \ref{t_reliability}        & & & & & X & & & & \\ \hline
Test \ref{t_usability}        & & & & & & X & & & \\ \hline
Test \ref{t_maintainability}        & & & & & & & X & & \\ \hline
Test \ref{t_portability}        & & & & & & & & X & \\ \hline
Test \ref{t_scalability}        & & & & & & & & & X \\ \hline
\end{tabular}
\caption{Traceability Between Test Cases and Requirements}
\label{Table:test_requirements}
\end{table}



% \wss{Provide a table that shows which test cases are supporting which
%   requirements.}

\section{Unit Test Description}\label{UTD}
This section will be filled in after the MIS (detailed design
  document) has been completed.

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
%\bibliographystyle{plainnat}

%\bibliography{../../refs/References}

\section*{Reference}
\begin{enumerate}
\item \refstepcounter{refnum} \label{ref1}
Mingsai Xu, Yuxin Zheng, Cancan Yang (2024). Assessing Highway Bridge Chloride Exposure at a Provincial Scale: Mapping and Projecting Impacts of Climate Change.  [Manuscript in preparation].

\end{enumerate}


\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions}\label{USQ}
\begin{itemize}
\item From one to five and five being the most satisfied, how would you rate your overall experience using the software?
\item Were the instructions clear and easy to understand. If no, please explain why.
\item Did the software include all the features you expected? If no, what additional features would you like to see?
\item From one to five and five being the most satisfied, how would you rate the speed and responsiveness of the software?
\end{itemize}


\end{document}