\documentclass[12pt, titlepage]{article}
\usepackage{amsmath, mathtools}
\usepackage{siunitx}
\usepackage{colortbl}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Software Requirements Specification for Bridge Corrosion: A Chloride Exposure Prediction Model} 
\author{Cynthia Liu}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb 9, 2024 & 1.0 & Initial release\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  BC & Bridge Corrosion\\
  FR & Functional Requirement\\
  MG & Module Guide\\
  MIS & Module Interface Specification\\
  NFR & Non-functional Requirement\\
  R & Requirement\\
  SRS & Software Requirements Specification\\
  T & Test\\
  VnV & Verification and Validation\\
  
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}
This document provides the road-map of the verification and validation plan for Bridge Corrosion (BC), and ensure the requirements and goals of the program mentioned in SRS. The organization of this document starts with the general information and verification plans, followed by the system test description for functional and non-functional requirements. Then, it includes the unit test which would not be filled in until after the MIS.


\section{General Information}

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}
The software BC, provides predictive trends for chloride exposure based on user-input coordinates within the province of Ontario. The software utilizes climate and traffic models to build a chloride exposure database, and search for the trend when user input location data.

\subsection{Objectives}
The objective of this document is to build confidence in the software's correctness and increase its realiability. All functional requirements and non-functional requirements mentioned in SRS are tested. We will try to cover a usability test if we have time and resource. We also assume the climate and traffic models we generate online have been verified by their implementation team so their accuracy is ensured.


\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

\wss{You should also list the objectives that are out of scope.  You don't have 
the resources to do everything, so what will you be leaving out.  For instance, 
if you are not going to verify the quality of usability, state this.  It is also 
worthwhile to justify why the objectives are left out.}

\wss{The objectives are important because they highlight that you are aware of 
limitations in your resources for verification and validation.  You can't do everything, 
so what are you going to prioritize?  As an example, if your system depends on an 
external library, you can explicitly state that you will assume that external library 
has already been verified by its implementation team.}

\subsection{Relevant Documentation}
The relevant documentation includes \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/ProblemStatementAndGoals/ProblemStatement.pdf}{Problem Statement} which is the proposed idea, \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/SRS/SRS.pdf}{Software Requirements Specifications} which outlines the requirement of the software. It would also be related to \href{}{VnV Report} which is a conclusion for validation and verification after the software is developed.

 
\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.}


\wss{Don't just list the other documents.  You should explain why they are relevant and 
how they relate to your VnV efforts.}

\section{Plan}
The VnV plan begins by introducing the verification and validation team (section \ref{VnVT}), followed by the verification plans for the SRS (section \ref{SRSVP}) and design (section \ref{DVP}). Subsequently, the verification plans for the VnV Plan (section \ref{VnVP}) and implementation (section \ref{IVP}) are presented. In the end, there are sections on automated testing and verification tools (Section \ref{ATVT}) and the software validation plan (section \ref{SVP}).

\wss{Introduce this section.   You can provide a roadmap of the sections to
  come.}

\subsection{Verification and Validation Team}\label{VnVT}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.   A table is a good way to summarize this information.}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{ %
    \begin{tabular}{ |l|l|p{2cm}|p{5cm}| } 
\hline        

	 Name & Document & Role & Description \\
\hline
      Dr. Spencer Smith & All & Instructor/ Reviewer & Review all the documents.  \\ \hline
      Mingsai Xu & - & Domain Expert & Provide theory support for the software.   \\ \hline     	  
	 Cynthia Liu & All & Author & Create all the documents, implement the software, and verify the implementation according to the VnV plan. \\ \hline
      Phil Du & All & Domain Expert Reviewer & Review all the documents. \\ \hline
      Hunter Ceranic & SRS & Secondary Reviewer & Review the SRS document. \\ \hline
      Fasil Cheema & VnV plan & Secondary Reviewer & Review the VnV plan.\\ \hline
      Fatemeh Norouziani & MG + MIS & Secondary Reviewer & Review the MG and MIS document. \\ \hline
\hline
\end{tabular} %
}
\caption{Verification and Validation Team}
\label{Table:VnVT}
\end{table}


\subsection{SRS Verification Plan}\label{SRSVP}
The SRS verification is done by peer review with classmates (Phil Du and Hunter Ceranic), giving suggestions by creating issues in Github. The author (Cynthia Liu) is responsible for reviewing and addressing the issues.\\
The SRS verification is also reviewed by Dr. Spencer Smith to offer any suggestions and feedbacks.\\
There is a \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS checklist} designed by Dr. Spencer Smith available to use if desired.
\wss{List any approaches you intend to use for SRS verification.  This may include
  ad hoc feedback from reviewers, like your classmates, or you may plan for 
  something more rigorous/systematic.}

\wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}\label{DVP}
The design verification, including the module guide (MG) and module interface specification (MIS), will be verfied by Phil Du and Fatemeh Norouziani. Dr. Spencer Smith will also review both documents. Reviewers will give feedbacks through Github issues and the author need to address them. The \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/Checklists/MG-Checklist.pdf}{MG checklist} and \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/Checklists/MIS-Checklist.pdf}{MIS checklist} designed by Dr. Spencer Smith could act as a help for reviewers.
\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}\label{VnVP}
The VnV plan is first created and verfied by the author, then turns to the team members to give any suggests by Github issues. The reviewers will access the \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/Checklists/VnV-Checklist.pdf}{VnV checklist} designed by Dr. Spencer Smith for reference.
\wss{The verification and validation plan is an artifact that should also be
verified.  Techniques for this include review and mutation testing.}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Implementation Verification Plan}\label{IVP}
The implementation would be verified by testing the functional requirements and non-functional requirements in section \ref{STD}. The unit tests will also be conducted as detailed in section \ref{UTD}. If possible, a code walkthrough could happen.
\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
  walkthroughs, code inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools}\label{ATVT}
The first part of the software, generating the database through climate and traffic model, will be done in MATLAB. The second part of finding the data for the input location, will be done in R. The automated testing will be done in MATLAB and R correspondingly. During the coding, the \href{https://www.mathworks.com/help/matlab/ref/mlint.html}{mlint} and \href{https://lintr.r-lib.org/}{lintr} will be used as a static code analysis. 
\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation Plan}\label{SVP}
Software validation plan is beyond the scope for BC as it involves extensive testing and validation against real-world data, which may require additional time, expertise, and resources that are not available within the scope of the project. 
\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
be used as an opportunity to validate the requirements.  You should plan on 
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose 
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}

\section{System Test Description}\label{STD}
\subsection{Tests for Functional Requirements}
Functional requirements for BC are given in \href{https://github.com/CynthiaLiu0805/BridgeCorrosion/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS checklist} section 5.1. There are five functional requirements for BC, R1 is related
to the input, R3 is related to the midway calculation, and the other requirements are corresponding to outputs. Section 5.1.1 describes the input tests related to R1, section 5.1.2 describes the intermediate test related to R3, and section 5.1.3 describes the output test related to other requirements.
R1: The user input need to be a coordinate within Ontario (By IM2).
R2: The output need to be a series of data showing the trend of chloride exposure in the
past and future at the input location (By IM2).
R3: During the calculation, the software should be capable of handling situations where
units do not match.
R4: The output from the previous year should be verifiable against real-world data.
R5: The output should be in two decimal points, showing the mass of chloride ions per unit
air volume (By IM1)


\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

\subsubsection{Input Test - test\_invalid\_input}
This section covers R1 of the SRS which includes the input coordinates check for the software. 

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Functional tests - Input tests - coordinate}

\begin{center}
\begin{table}[h]
\resizebox{\textwidth}{!}
{ %
    \begin{tabular}{ lccccc }
    \hline
      \multicolumn{2}{c|}{Input (\si[per-mode=symbol] {\degree}) }                            & \multicolumn{2}{c}{Output} \\ 
    
    \hline

        $longitude$   &   $latitude$     &   valid?   &   Error Message \\ \hline
    
       0.039  & 0.046&  Y  & NONE                         \\
       0.039  & 0.046&  Y  & NONE                         \\
       0.039  & 0.046&  Y  & NONE                         \\
       0.039  & 0.046&  Y  &  Error Message                         \\
       0.039  & 0.046&  Y  &  Error Message                         \\
       0.039  & 0.046&  Y  &  Error Messagevsdgdfsgfas                         \\
    
    \hline
    
    
    \end{tabular} %
}
\caption{Coordinate input tests}
\label{Table:coordinate_test}
\end{table}
\end{center}

\begin{enumerate}

\item{test-input-coordinate\\}

Control: Automatic
					
Initial State: Uninitialized
					
Input: The input column in Table \ref{Table:coordinate_test}.
					
Output: Either gives an error message or process to next step as in Table \ref{Table:coordinate_test}. \wss{The expected result for the given inputs}

Test Case Derivation: The software produces error message for the invalid inputs, for the valid inputs, it will proceed to next step. \wss{Justify the expected value given in the Output field}
					
How test will be performed: The automatic test will be perfirmed by R.
\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}
NFR1: Reliability: The predictions generated by the software should be accurate and reliable,
reflecting real-world conditions and factors influencing chloride exposure.
NFR2: Usability: The software interface should be intuitive and user-friendly, allowing users
in the section 3.2 to easily input coordinates and look at the predicted chloride exposure
in the past and future.
NFR3: Maintainability: The code for this software should be designed and structured in a
way that it could be easily comprehended and modified by other potential developers.
NFR4: Portability: This software should be able to run on recent versions of Google Chrome,
Firefox, MS Edge and Safari. The operating system include Windows 7+ and Mac OS
X 10.7+.
NFR5: Scalability: The software should be scalable to accommodate potential
\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\subsubsection{Area of Testing1}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}\label{UTD}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.  Please answer the following questions:

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.  Please answer the following questions:

\begin{enumerate}
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}